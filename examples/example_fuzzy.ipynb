{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huff, Puff & Classify\n",
    "### Example Notebook: Fuzzy String Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Package imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os, os.path\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "#import custom modules\n",
    "sys.path.append('../hp_classify')\n",
    "import prep.prep_data as prep\n",
    "import prep.prep_cv as cv\n",
    "import model.fuzzy as fz\n",
    "import model.cv as fzcv\n",
    "\n",
    "#magik\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User Globals\n",
    "#TODO: Add instructions for user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup globals\n",
    "#setup directories\n",
    "CWD = os.getcwd()\n",
    "HOME_DIR = os.path.abspath(os.path.join(CWD, os.pardir))\n",
    "DATA_DIR = HOME_DIR + \"/data\"\n",
    "DATA_FILENAME = \"housing_data.csv\"\n",
    "RESULTS_DIR = HOME_DIR + \"/results\"\n",
    "\n",
    "#script options\n",
    "WRITE_FILES = False #do you want to write output csvs of results?\n",
    "\n",
    "#setup lists of vars to work with\n",
    "STR_VARS = ['housing_roof', 'housing_wall', 'housing_floor']\n",
    "NUM_VARS = [s + '_num' for s in STR_VARS]\n",
    "RANK_VARS = [s + '_rank' for s in STR_VARS]\n",
    "\n",
    "#which variable do you want to predictn (currently: floor/wall/roof)\n",
    "DEP_VAR = \"housing_roof\"\n",
    "PRED_VAR = DEP_VAR + \"_rank\" #will always be using the strings to predict ranking\n",
    "\n",
    "#setup a filter to select which surveys you want to work with\n",
    "SVY_FILTER = ['MACRO_DHS']\n",
    "\n",
    "#analytical options\n",
    "CV_SAMPLE_PCT = .2 #hold out x% for testing\n",
    "CV_SAMPLE_WT = \"N\" #which variable(if any) shall weight your test sample\n",
    "CV_FOLDS = 2 #use a x-fold cross-validation env\n",
    "\n",
    "#garbage lists\n",
    "STR_GARBAGE = ['nan', 'other', 'not a dejure resident', 'not dejure resident']\n",
    "RANK_GARBAGE = ['4', '5', '6', '7', '8', '9', 'n']\n",
    "\n",
    "#dictionaries\n",
    "PRED_DICT = {'natural':'1', 'rudimentary':'2', 'finished':'3'} #map categories back to ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in and prep the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~begin reading\n",
      "data read!\n",
      "~begin cleaning\n"
     ]
    }
   ],
   "source": [
    "#read in data and clean the text, then subset to our gold standard MACRO DHS data\n",
    "df = prep.read_then_clean(DATA_DIR + \"/\" + DATA_FILENAME, STR_VARS, SVY_FILTER)\n",
    "\n",
    "#remove garbage codes from our string variables\n",
    "df_clean = prep.remove_garbage_codes(df, STR_VARS, STR_GARBAGE)\n",
    "\n",
    "#extract ranking values for roof/wall/floor, then remove non-informative values\n",
    "#note that we only want to use ranks from 1-3, as these correspond to our final output ranks\n",
    "df_clean = prep.extract_ranking(df_clean, NUM_VARS)\n",
    "df_clean = prep.remove_garbage_codes(df_clean, RANK_VARS, RANK_GARBAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run a bifold cross validation exercise for \"housing roof\"\n",
    "##### Since this is an example, we will only work on 25 unknown words from each cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling df, iteration # 0\n",
      "sampling df, iteration # 1\n",
      "sampling df, iteration # 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a409141c03234d05bd0629feb5ab725a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='cv loop', max=3, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on cv loop # 0\n",
      "building corpus for rank # 1\n",
      "building corpus for rank # 2\n",
      "building corpus for rank # 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "classifying unknown strings:   0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting unknown strings\n",
      "need to classify 285 unknown strings\n",
      "analyzing... cardboard cloth tent\n",
      "~>corpus# 0\n",
      "~>corpus# 1\n",
      "~>corpus# 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "classifying unknown strings:   4%|▍         | 1/25 [00:39<15:38, 39.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing... cement bricks\n",
      "~>corpus# 0\n",
      "~>corpus# 1\n",
      "~>corpus# 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "classifying unknown strings:   8%|▊         | 2/25 [01:15<14:41, 38.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing... sod mud with grass\n",
      "~>corpus# 0\n",
      "~>corpus# 1\n",
      "~>corpus# 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "classifying unknown strings:  12%|█▏        | 3/25 [01:52<13:56, 38.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing... thatch bushes grass\n",
      "~>corpus# 0\n",
      "~>corpus# 1\n",
      "~>corpus# 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "classifying unknown strings:  16%|█▌        | 4/25 [02:31<13:20, 38.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing... wood planks\n",
      "~>corpus# 0\n",
      "~>corpus# 1\n",
      "~>corpus# 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c75061010d1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#run your cross-validation analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m cv_distrib, cv_preds, cv_results, cv_df = fzcv.fuzzy_cv(train_list, DEP_VAR, PRED_DICT,\n\u001b[0;32m----> 7\u001b[0;31m                                                         subset=range(25)) #only run on 25 words from each CV for speed\n\u001b[0m\u001b[1;32m      8\u001b[0m                                                         \u001b[0;31m#jupyter=True) #display jupyter progress bar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/_code/school/cse583/hpc-housing-quality/hp_classify/model/cv.py\u001b[0m in \u001b[0;36mfuzzy_cv\u001b[0;34m(cv_list, base_var, rank_dictionary, subset, threshold)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m#find distribution of scores for each string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mdistrib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuzzy_scan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midk_strings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m#TODO, output plots of distribution for analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/_code/school/cse583/hpc-housing-quality/hp_classify/model/fuzzy.py\u001b[0m in \u001b[0;36mfuzzy_scan\u001b[0;34m(unknown_list, corpus_list, jupyter)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;31m# loop over each word and compute the similarity score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mz\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuzz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWRatio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munknown_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# append scores to create a distribution for the entire corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hpc/lib/python3.7/site-packages/fuzzywuzzy/fuzz.py\u001b[0m in \u001b[0;36mWRatio\u001b[0;34m(s1, s2, force_ascii, full_process)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mptsor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mptser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         \u001b[0mtsor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_sort_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_process\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0munbase_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m         \u001b[0mtser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_set_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_process\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0munbase_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hpc/lib/python3.7/site-packages/fuzzywuzzy/fuzz.py\u001b[0m in \u001b[0;36mtoken_sort_ratio\u001b[0;34m(s1, s2, force_ascii, full_process)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mbut\u001b[0m \u001b[0msorting\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0mbefore\u001b[0m \u001b[0mcomparing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_token_sort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_ascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_ascii\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_process\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull_process\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hpc/lib/python3.7/site-packages/fuzzywuzzy/utils.py\u001b[0m in \u001b[0;36mdecorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hpc/lib/python3.7/site-packages/fuzzywuzzy/fuzz.py\u001b[0m in \u001b[0;36m_token_sort\u001b[0;34m(s1, s2, partial, force_ascii, full_process)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_token_sort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_ascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_process\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0msorted1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_and_sort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_ascii\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_process\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull_process\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0msorted2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_and_sort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_ascii\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_process\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull_process\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hpc/lib/python3.7/site-packages/fuzzywuzzy/fuzz.py\u001b[0m in \u001b[0;36m_process_and_sort\u001b[0;34m(s, force_ascii, full_process)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;31m# pull tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_ascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_ascii\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfull_process\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;31m# sort tokens and join\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#setup training/test data for a bifold cross validation, using the 'housing roof\" variable to predict \n",
    "train_list = cv.cv_censor_col(df_clean, PRED_VAR, CV_SAMPLE_PCT, CV_SAMPLE_WT, CV_FOLDS)\n",
    "\n",
    "#run bifold cross validation for \"housing roof\"\n",
    "#run your cross-validation analysis\n",
    "cv_distrib, cv_preds, cv_results, cv_df = fzcv.fuzzy_cv(train_list, DEP_VAR, PRED_DICT,\n",
    "                                                        subset=range(25), #only run on 25 words from each CV for speed\n",
    "                                                        jupyter=True) #display jupyter progress bar\n",
    "\n",
    "#output the results to csv\n",
    "if WRITE_FILES == True:\n",
    "    save_results_df(cv_results, out_dir, \"ex_cv_results\")\n",
    "    save_results_df(cv_preds, out_dir, \"ex_cv_preds\")\n",
    "    save_results_df(cv_df, out_dir, \"ex_cv_df\")\n",
    "    save_results_df(cv_distrib, out_dir, \"ex_cv_distrib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot results\n",
    "fz.fuzzy_density(pd.concat(cv_distrib), 'word', \n",
    "                 ['natural', 'rudimentary', 'finished'],\n",
    "                 color_list={'natural':'r', 'rudimentary':'b', 'finished':'g'},\n",
    "                 cutoff=75)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
